{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbb470b-f324-488f-8e5c-5dc95c378de4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a4a67-a320-49fb-bad8-877b432a374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing the EDGAR tool\n",
    "!pip install --user edgartools\n",
    "\n",
    "#Installing openai and tiktoken\n",
    "pip install --user openai\n",
    "!pip install tiktoken\n",
    "\n",
    "#OpenAI update\n",
    "!pip install --upgrade openai\n",
    "\n",
    "#Installing parser library\n",
    "!pip install lxml\n",
    "\n",
    "#Installing imbalanced\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4a795-c341-4661-ae96-5c7e648acc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all necessary modules\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import re\n",
    "import datetime\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from edgar import *\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff290b1-37c6-4e7c-89f2-42ff94d00a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting identity\n",
    "from edgar import set_identity\n",
    "set_identity(\"xxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eacb6b-0310-47db-a8f5-a5a0a51c78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fd2bb-14e4-4be1-a7af-cf8fae587557",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f2c55-9b6e-465b-b935-60c1d67de7a3",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/\n",
    "https://pandas.pydata.org/\n",
    "https://numpy.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4cf72-cd32-447c-a255-490f0969f63c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fecdc-04ea-40a5-b1f5-c6485f606f0e",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Parsing Text:\n",
    "https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "Tiktoken:\n",
    "https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a6342-bdd9-47a5-8434-f2ca02c3a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing Text\n",
    "def parsing_text(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    parsed_text = soup.get_text(' ', strip = True)\n",
    "    return parsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ebd94-cb8d-4c76-922d-c2ab0e01c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting Tokens\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b23c1-d745-4ff2-9b1b-2782835f3533",
   "metadata": {},
   "source": [
    "## Tickers to CIK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc68b6-b1a2-47b2-a6e1-a02a1946f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import NYSE ciks\n",
    "tickersNYSE =[]\n",
    "\n",
    "with open('c:/Users/xxxxx/Downloads/tiNYSE.csv', 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        tickersNYSE.append(row[0])\n",
    "        \n",
    "tickersNYSE.remove('TICKER')\n",
    "\n",
    "tickersNYSEa = tickersNYSE[:1750]\n",
    "tickersNYSEb = tickersNYSE[1750:]\n",
    "\n",
    "print(len(tickersNYSE))\n",
    "print(len(tickersNYSEa))\n",
    "print(len(tickersNYSEb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb4bf2-6b73-48a4-9bfb-85932fd1a4e0",
   "metadata": {},
   "source": [
    "Matching the tickers to a CIK in the EDGAR database and writing to a ciksA and ciksB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bcd9a-69dc-46a1-9e0d-3f814af92e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding CIK numbers\n",
    "\n",
    "raw_tickers = []\n",
    "ciks_a = []\n",
    "\n",
    "for k in tickersNYSEb:\n",
    "    company = Company(k)\n",
    "    if company != None:\n",
    "        raw_tickers.append(k)\n",
    "        \n",
    "for j in raw_tickers:\n",
    "    cik = Company(j).cik\n",
    "    if cik != None:\n",
    "        ciks_a.append(cik)\n",
    "\n",
    "with open('c:/Users/xxxxx/Downloads/ciksA.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(ciks_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b736a-3d05-44a8-a1a9-b1072da33d10",
   "metadata": {},
   "source": [
    "Opening the two csv files and writing to a list. Combining the lists and writing to a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216b47d-ae57-458f-9ab3-3f9c83233a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ciksA = []\n",
    "ciksB = []\n",
    "\n",
    "\n",
    "with open('c:/Users/xxxxx/Downloads/ciksA.csv', mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    # Iterate through each row in the CSV file and append it to the list\n",
    "    for row in reader:\n",
    "        ciks_total.extend(row)\n",
    "        \n",
    "with open('c:/Users/xxxxx/Downloads/ciksB.csv', mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    # Iterate through each row in the CSV file and append it to the list\n",
    "    for row in reader:\n",
    "        ciks_total.extend(row)\n",
    "        \n",
    "ciks_combined = ciksA + ciksB\n",
    "\n",
    "\n",
    "with open('c:/Users/xxxxx/Downloads/ciks_numbers.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(ciks_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e7275-783b-49ff-a254-27898008ed6f",
   "metadata": {},
   "source": [
    "## Getting the filings from the EDGAR Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ef97b-f3c7-4c10-b0c8-8bc2533d4540",
   "metadata": {},
   "source": [
    "References:\n",
    "https://pypi.org/project/edgartools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97406a-04d0-483d-8d38-162d9287d3af",
   "metadata": {},
   "source": [
    "Getting each year seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d126e-8f83-49b3-b73c-14dc7800bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing ciks\n",
    "\n",
    "ciks_total = []\n",
    "ciks_numbers = []\n",
    "\n",
    "\n",
    "with open('c:/Users/xxxxx/Downloads/ciks_numbers.csv', mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    # Iterate through each row in the CSV file and append it to the list\n",
    "    for row in reader:\n",
    "        ciks_total.extend(row)\n",
    "\n",
    "for c in ciks_total:\n",
    "    ciks_numbers.append(int(c))\n",
    "\n",
    "\n",
    "print(len(ciks_total))\n",
    "\n",
    "\n",
    "#Get filings per Quarter\n",
    "filings_2018_1 = get_filings(2018, 1, form=[\"8-K\"])\n",
    "\n",
    "\n",
    "\n",
    "Dict_Filter = {}\n",
    "for i in range(len(filings_2018_1)):\n",
    "    company_pdate= filings_2018_1[i]\n",
    "    cik = company_pdate.cik\n",
    "    if cik in ciks_numbers:\n",
    "        company_date = company_pdate.filing_date\n",
    "        key_p = str(cik) + '_' + str(company_date)\n",
    "        company_text= company_pdate.text()\n",
    "        company_final = company_text.split('</body>')[0].lstrip().split('</body>')[0]\n",
    "        company_parsed = parsing_text(company_final)\n",
    "        if key_p not in Dict_Filter:\n",
    "            Dict_Filter[key_p] = {}\n",
    "        Dict_Filter[key_p] = company_parsed\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#Saving the output to csv\n",
    "# open file for writing, \"w\" is writing\n",
    "w = csv.writer(open(\"c:/Users/xxxxx/Downloads/1st2018.csv\", \"w\",encoding='utf-8'))\n",
    "\n",
    "# loop over dictionary keys and values\n",
    "for key, val in Dict_Filter.items():\n",
    "\n",
    "    # write every key and value to file\n",
    "    w.writerow([key, val])\n",
    "\n",
    "filings_2018_1 = get_filings(2018, 2, form=[\"8-K\"])\n",
    "\n",
    "Dict_Filter = {}\n",
    "for i in range(len(filings_2018_1)):\n",
    "    company_pdate= filings_2018_1[i]\n",
    "    cik = company_pdate.cik\n",
    "    if cik in ciks_numbers:\n",
    "        company_date = company_pdate.filing_date\n",
    "        key_p = str(cik) + '_' + str(company_date)\n",
    "        company_text= company_pdate.text()\n",
    "        company_final = company_text.split('</body>')[0].lstrip().split('</body>')[0]\n",
    "        company_parsed = parsing_text(company_final)\n",
    "        if key_p not in Dict_Filter:\n",
    "            Dict_Filter[key_p] = {}\n",
    "        Dict_Filter[key_p] = company_parsed\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#Saving the output to csv\n",
    "# open file for writing, \"w\" is writing\n",
    "w = csv.writer(open(\"c:/Users/xxxxx/Downloads/2nd2018.csv\", \"w\",encoding='utf-8'))\n",
    "\n",
    "# loop over dictionary keys and values\n",
    "for key, val in Dict_Filter.items():\n",
    "\n",
    "    # write every key and value to file\n",
    "    w.writerow([key, val])\n",
    "\n",
    "filings_2018_1 = get_filings(2018, 3, form=[\"8-K\"])\n",
    "\n",
    "Dict_Filter = {}\n",
    "for i in range(len(filings_2018_1)):\n",
    "    company_pdate= filings_2018_1[i]\n",
    "    cik = company_pdate.cik\n",
    "    if cik in ciks_numbers:\n",
    "        company_date = company_pdate.filing_date\n",
    "        key_p = str(cik) + '_' + str(company_date)\n",
    "        company_text= company_pdate.text()\n",
    "        company_final = company_text.split('</body>')[0].lstrip().split('</body>')[0]\n",
    "        company_parsed = parsing_text(company_final)\n",
    "        if key_p not in Dict_Filter:\n",
    "            Dict_Filter[key_p] = {}\n",
    "        Dict_Filter[key_p] = company_parsed\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#Saving the output to csv\n",
    "# open file for writing, \"w\" is writing\n",
    "w = csv.writer(open(\"c:/Users/xxxxx/Downloads/3rd2018.csv\", \"w\",encoding='utf-8'))\n",
    "\n",
    "# loop over dictionary keys and values\n",
    "for key, val in Dict_Filter.items():\n",
    "\n",
    "    # write every key and value to file\n",
    "    w.writerow([key, val])\n",
    "\n",
    "\n",
    "filings_2018_1 = get_filings(2018, 4, form=[\"8-K\"])\n",
    "\n",
    "Dict_Filter = {}\n",
    "for i in range(len(filings_2018_1)):\n",
    "    company_pdate= filings_2018_1[i]\n",
    "    cik = company_pdate.cik\n",
    "    if cik in ciks_numbers:\n",
    "        company_date = company_pdate.filing_date\n",
    "        key_p = str(cik) + '_' + str(company_date)\n",
    "        company_text= company_pdate.text()\n",
    "        company_final = company_text.split('</body>')[0].lstrip().split('</body>')[0]\n",
    "        company_parsed = parsing_text(company_final)\n",
    "        if key_p not in Dict_Filter:\n",
    "            Dict_Filter[key_p] = {}\n",
    "        Dict_Filter[key_p] = company_parsed\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#Saving the output to csv\n",
    "# open file for writing, \"w\" is writing\n",
    "w = csv.writer(open(\"c:/Users/xxxxx/Downloads/4th2018.csv\", \"w\",encoding='utf-8'))\n",
    "\n",
    "# loop over dictionary keys and values\n",
    "for key, val in Dict_Filter.items():\n",
    "\n",
    "    # write every key and value to file\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8d82d-d756-43a5-a3fe-93ed73875979",
   "metadata": {},
   "source": [
    "## Getting the filings for transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ee004-63de-46fd-9797-28e02eff36c8",
   "metadata": {},
   "source": [
    "References:\n",
    "https://pypi.org/project/edgartools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb5443-41d6-4091-b260-39836ba74b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('c:/Users/xxxxx/Downloads/Thesis Data Science/Data/SDC/ciks_transactions_NYSE.csv', mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    # Iterate through each row in the CSV file and append it to the list\n",
    "    for row in reader:\n",
    "        ciks_total.extend(row)\n",
    "\n",
    "for c in ciks_total:\n",
    "    ciks_numbers.append(int(c))\n",
    "\n",
    "\n",
    "print(len(ciks_total))\n",
    "\n",
    "\n",
    "ciksA = ciks_numbers[:296]\n",
    "ciksB = ciks_numbers[296:]\n",
    "\n",
    "print(len(ciksA))\n",
    "print(len(ciksB))\n",
    "\n",
    "Dict_Filter={}\n",
    "for cik in ciksA:\n",
    "    company = Company(cik)\n",
    "    if company is not None:\n",
    "        company_p = company.get_filings(form=\"8-K\").filter(date=\"2017-12-31:2022-12-31\")\n",
    "        if company_p is not None:\n",
    "            for i in range(len(company_p)):\n",
    "                company_pdate= company_p[i]\n",
    "                company_date = company_pdate.filing_date\n",
    "                key_p = str(cik) + '_' + str(company_date)\n",
    "                company_text= company_pdate.text()\n",
    "                company_final = company_text.split('</body>')[0].lstrip().split('</body>')[0]\n",
    "                company_parsed = parsing_text(company_final)\n",
    "                if key_p not in Dict_Filter:\n",
    "                    Dict_Filter[key_p] = {}\n",
    "                Dict_Filter[key_p] = company_parsed\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "#Saving the output to csv\n",
    "# open file for writing, \"w\" is writing\n",
    "w = csv.writer(open(\"c:/Users/xxxxx/Downloads/filings_SDC_NYSE_A.csv\", \"w\",encoding='utf-8'))\n",
    "\n",
    "# loop over dictionary keys and values\n",
    "for key, val in Dict_Filter.items():\n",
    "\n",
    "    # write every key and value to file\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46812d68-293d-4793-9b47-7d33330724e7",
   "metadata": {},
   "source": [
    "## Converting to a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993fd3c-90f1-475a-99e7-5023ddbc007a",
   "metadata": {},
   "source": [
    "For each year and for the transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bcf4a-e2ee-4574-a052-0ef4a887f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(100000000)\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "Dict_1_2018 = {}\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(\"c:/Users/xxxxx/Downloads/Thesis Data Science/Output/1st2018.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        if len(row) == 2:\n",
    "            cik, value = row\n",
    "            Dict_1_2018[cik] = value\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "Dict_2_2018 = {}\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(\"c:/Users/xxxxx/Downloads/Thesis Data Science/Output/2nd2018.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        if len(row) == 2:\n",
    "            cik, value = row\n",
    "            Dict_2_2018[cik] = value\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "Dict_3_2018 = {}\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(\"c:/Users/xxxxx/Downloads/Thesis Data Science/Output/3rd2018.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        if len(row) == 2:\n",
    "            cik, value = row\n",
    "            Dict_3_2018[cik] = value\n",
    "            \n",
    "# Create an empty dictionary to store the data\n",
    "Dict_4_2018 = {}\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(\"c:/Users/xxxxx/Downloads/Thesis Data Science/Output/4th2018.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        if len(row) == 2:\n",
    "            cik, value = row\n",
    "            Dict_4_2018[cik] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dacd8-45ec-4b7d-89ec-080c964a22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.DataFrame(Dict_1_2018, index=['combined'])\n",
    "df_2018_1 = dft.T\n",
    "df_2018_1.reset_index(inplace=True)\n",
    "df_2018_1[['cik','date']] = df_2018_1['index'].str.split('_', expand=True)\n",
    "df_2018_1['cik'] = df_2018_1['cik'].astype(int)\n",
    "\n",
    "\n",
    "dft = pd.DataFrame(Dict_2_2018, index=['combined'])\n",
    "df_2018_2 = dft.T\n",
    "df_2018_2.reset_index(inplace=True)\n",
    "df_2018_2[['cik','date']] = df_2018_2['index'].str.split('_', expand=True)\n",
    "df_2018_2['cik'] = df_2018_2['cik'].astype(int)\n",
    "\n",
    "\n",
    "dft = pd.DataFrame(Dict_3_2018, index=['combined'])\n",
    "df_2018_3 = dft.T\n",
    "df_2018_3.reset_index(inplace=True)\n",
    "df_2018_3[['cik','date']] = df_2018_3['index'].str.split('_', expand=True)\n",
    "df_2018_3['cik'] = df_2018_3['cik'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "dft = pd.DataFrame(Dict_4_2018, index=['combined'])\n",
    "df_2018_4 = dft.T\n",
    "df_2018_4.reset_index(inplace=True)\n",
    "df_2018_4[['cik','date']] = df_2018_4['index'].str.split('_', expand=True)\n",
    "df_2018_4['cik'] = df_2018_4['cik'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee826a-1dba-4933-a32b-c9701de9cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018_1 = pd.concat([df_2018_1,df_2018_2], ignore_index=True)\n",
    "df_2018_2 = pd.concat([df_2018_3,df_2018_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cd5b7-1ac4-4c3a-92d7-71c8cbdd0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018_1.to_csv('c:/Users/xxxxx/Downloads/dfNYSE_2018_1.csv', index=False)\n",
    "df_2018_2.to_csv('c:/Users/xxxxx/Downloads/dfNYSE_2018_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda0fb3-a15c-4aab-ab5d-f956ca9c2658",
   "metadata": {},
   "source": [
    "## Getting the transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a6302-c873-460a-8039-83af9be5b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Deals dataframe\n",
    "df_tickerdeals = pd.read_csv('c:/Users/xxxxx/Downloads/Deals & Dates NYSE.csv')\n",
    "\n",
    "#Removing where CIK is 0\n",
    "df_cik_transactions = df_tickerdeals[df_tickerdeals['cik'] != 0]\n",
    "\n",
    "#Transforming the date to the format\n",
    "df_cik_transactions['DATE'] = pd.to_datetime(df_cik_transactions['DATE'], format='%d/%m/%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#Writing to a file\n",
    "df_cik_transactions.to_csv('c:/Users/xxxxx/Downloads/df_trans_NYSE_adj.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5b6c9-4eb8-490e-bf47-31ab7ed3a626",
   "metadata": {},
   "source": [
    "## Removing large filings and matching the dataframe on CIKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac720db-03af-449a-b68e-0cdf942eb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the NYSE Dataframes\n",
    "df_combined= pd.read_csv('c:/Users/xxxxx/Downloads/df_NYSE_2018_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6d154-0d6c-47e4-8571-0e4bb3d0b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting tokens\n",
    "df_combined['Tokens'] = df_combined['combined'].apply(lambda x: num_tokens_from_string(x, \"cl100k_base\"))\n",
    "\n",
    "\n",
    "# omit 8-K files that are too long to embed\n",
    "count_of_8000 = (df_combined['Tokens'] > 8000).sum()\n",
    "\n",
    "print(count_of_8000)\n",
    "\n",
    "condition_tokens = (df_combined['Tokens'] > 8000) \n",
    "df_combined = df_combined[~condition_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4dd5d-dd5a-48c6-8339-5adc82750e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Deals tickers\n",
    "df_cik_transactions = pd.read_csv('c:/Users/xxxxx/Downloads/df_trans_NYSE_adj.csv')\n",
    "\n",
    "\n",
    "#Adding transactions to dataframe\n",
    "df_combined = df_combined.merge(df_cik_transactions, on='cik', how='left')\n",
    "\n",
    "\n",
    "#Setting Transaction to 1\n",
    "df_combined['Transaction'] = 1\n",
    "df_combined.loc[df_combined['TICKER'].isna(), 'Transaction'] = 0\n",
    "\n",
    "count_of_ones = (df_combined['Transaction'] == 1).sum()\n",
    "\n",
    "#Removing after dates\n",
    "df_combined = df_combined.sort_values(by = ['cik','date'])\n",
    "\n",
    "condition = (df_combined['Transaction'] == 1) & (df_combined['date'] > df_combined['DATE'])\n",
    "df_combined= df_combined[~condition]\n",
    "\n",
    "#Writing to a file\n",
    "\n",
    "df_combined.to_csv('c:/Users/xxxxx/Downloads/shortened/df_NYSE_2018_1_short.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d77c8-50af-4a15-ba77-24dc8199210d",
   "metadata": {},
   "source": [
    "## Combining Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc651a-c58d-436a-b7a9-3d01c5058e38",
   "metadata": {},
   "source": [
    "Creating 4 combined files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653c4be-da05-428b-a5f6-c33fcb7c1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('c:/Users/xxxxx/Downloads/df_NYSE_2018_1_short.csv')\n",
    "df_2 = pd.read_csv('c:/Users/xxxxx/Downloads/df_NYSE_2018_1_short.csv')\n",
    "\n",
    "df_combined = [pd.concat([df_1,df_2], ignore_index=True)\n",
    "\n",
    "df_combined.to_csv('c:/Users/xxxxx/Downloads/df_NYSE_1_short.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb60875-2a24-4ee0-8a63-34e4de748481",
   "metadata": {},
   "source": [
    "## Embedding the filings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1195e91-7766-4fab-a400-94e2309036b6",
   "metadata": {},
   "source": [
    "References:\n",
    "https://pypi.org/project/edgartools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cba096-9d15-4b7f-a158-6e386d7ab41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the dataframe\n",
    "df_combined = pd.read_csv('c:/Users/xxxxx/Downloads/df_NYSE_4_short.csv')\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  \n",
    "\n",
    "\n",
    "#Obtaining the Embeddings\n",
    "api_key  = \"xxxxxxxxxxxxxxxxxx\"\n",
    "client = OpenAI(api_key= api_key)\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "df_combined['ada_embedding'] = df_combined.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "\n",
    "#Write to a file\n",
    "df_combined.to_csv('c:/Users/xxxxx/Downloads/Thesis Data Science/Embedded/df_NYSE_embedded_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b688fc-f494-4af7-89bd-76c2dc2dc03a",
   "metadata": {},
   "source": [
    "Upload to drive and use google collab to use GPU's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b6cf9-1277-4cd5-9af8-5590edd3ba97",
   "metadata": {},
   "source": [
    "## Cutting down Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745db153-398e-493a-9eae-d7b25f6e2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_1 = pd.read_csv('/content/drive/My Drive/Thesis Backup/df_NYSE_embedded_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddaddec-e9bb-4b14-a310-52286ea2cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_final_1,df_final_2, df_final_3, df_final_4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae2804-3e9b-4b52-b1c9-3402c40420f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing Size\n",
    "#Gettingtarget and non-target\n",
    "\n",
    "grouped = df_final.groupby('Transaction')\n",
    "\n",
    "df_acquired =  grouped.get_group(1)\n",
    "df_not_acquired =  grouped.get_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729e85f-75f3-4793-9d00-90f46f093cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting number of Targets and Non-Targets in Final Dataset\n",
    "\n",
    "uniqueValues = df_final['cik'].nunique()\n",
    "uniqueValues1 = df_acquired['cik'].nunique()\n",
    "uniqueValues2 = df_not_acquired['cik'].nunique()\n",
    "print(uniqueValues)\n",
    "print(uniqueValues1, uniqueValues2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c270e1-9534-4b90-bd40-f554eb87df85",
   "metadata": {},
   "source": [
    "## Rebalancing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d03c08-5d36-4c4a-a20e-675e43535a60",
   "metadata": {},
   "source": [
    "References:\n",
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "https://cookbook.openai.com/examples/classification_using_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f2807-1262-4f28-bd5e-383887dd32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting X, y and companies\n",
    "X_s = df_final.drop(['date','Transaction','Tokens','combined', 'DATE','TICKER','Unnamed: 0','index'], axis = 1)\n",
    "y=  df_final['Transaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73061a91-dcc1-4c82-bd8d-1fb7794bfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data based on ciks\n",
    "train_ciks, test_ciks = train_test_split(df_final['cik'].unique(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e6c4c-d6e8-43ad-840a-f84b9dae11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a boolian discriminator\n",
    "train_mask = df_final['cik'].isin(train_ciks)\n",
    "test_mask = df_final['cik'].isin(test_ciks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524a047-6b87-4b13-9104-df9bdf5132a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Dataset\n",
    "X_train_s = X_s[train_mask]\n",
    "y_train = y[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf3f4f-7a23-47fa-9efe-f5bdf13e63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_s = X_s[test_mask]\n",
    "y_test = y[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f68b4-0b30-4ddf-8128-eaa8b93a6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s.to_csv('/content/drive/My Drive/Thesis Backup/X_train_s.csv', index = False)\n",
    "X_test_s.to_csv('/content/drive/My Drive/Thesis Backup/X_test_s.csv', index = False)\n",
    "y_train.to_csv('/content/drive/My Drive/Thesis Backup/y_train.csv', index = False)\n",
    "y_test.to_csv('/content/drive/My Drive/Thesis Backup/y_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96139731-15bb-40ad-8ec0-96f0fd282d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = pd.read_csv('/content/drive/My Drive/Thesis Backup/X_train_s.csv')\n",
    "X_test_s = pd.read_csv('/content/drive/My Drive/Thesis Backup/X_test_s.csv')\n",
    "y_train = pd.read_csv('/content/drive/My Drive/Thesis Backup/y_train.csv')\n",
    "y_test = pd.read_csv('/content/drive/My Drive/Thesis Backup/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644b4bf-1dbf-4c2a-8f2d-39a092c945c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "X_train_np = X_train_s.ada_embedding.apply(eval).apply(np.array)\n",
    "X_test_np = X_test_s.ada_embedding.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4d210-6bd0-4378-8fa6-2a141f99e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(X_train_np.values)\n",
    "X_test = list(X_test_np.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc08d9-a58a-46f7-b079-aa28c26d270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with imbalanced data\n",
    "over = SMOTE(sampling_strategy= 0.55)\n",
    "under = RandomUnderSampler(sampling_strategy= 1.00)\n",
    "\n",
    "steps= [('o', over),('u',under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c42ea2-f301-44d0-ace2-897b7026baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded0316-189d-4ce9-87de-8d15946ee4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Distribution\n",
    "classifier = LogisticRegression(random_state = 42, penalty = 'l2', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a29e5c-2738-4600-861d-a1db04b7b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efcc66-f0b0-42d6-aee3-310a1336e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using cross-validation\n",
    "scores = cross_val_score(classifier, X_resampled, y_resampled['Transaction'], cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354070a0-88b5-448f-ba78-7d930b63b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC-ROC Cross-Validation Results:\")\n",
    "print(f\"AUC-ROC: {scores.mean():.4f} (Â±{scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e7a0e-c0b2-4bc7-80a1-7a364642ebe2",
   "metadata": {},
   "source": [
    "## Using Final Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdf29f-a7d8-460a-b1e4-fe5be0c054a7",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8124e2-2cbe-4971-a2bf-801e9a405e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the entire resampled training data\n",
    "classifier.fit(X_resampled, y_resampled['Transaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66c62e-13f5-4654-abec-9adc8033f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662e737-e2bb-424d-ac42-bfaeb2df09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Logistic Regression\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr)\n",
    "fpr_lr, tpr_lr, thresholds = roc_curve(y_test , y_pred_lr)\n",
    "AUC_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_lr:.2f}\")\n",
    "print(f\"Precision: {precision_lr:.2f}\")\n",
    "print(f\"AUC: {AUC_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5eb93-af9c-4020-bb55-513e522389d1",
   "metadata": {},
   "source": [
    "Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11462b00-5f0c-43a7-868f-eecb9866a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training SVM-Linear Model\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_resampled, y_resampled['Transaction'])\n",
    "y_pred_svm = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e433b-acf7-45fb-84f8-40745f392225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training SVM-Linear Model\n",
    "clf = svm.LinearSVC(C = 0.01, max_iter = 1000)\n",
    "clf.fit(X_resampled, y_resampled['Transaction'])\n",
    "y_pred_svm = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0385f4e-0a4d-4320-b9f2-33de56c3e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing SVM\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "fpr_svm, tpr_svm, thresholds = roc_curve(y_test , y_pred_svm)\n",
    "AUC_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "print(f\"Accuracy SVM: {accuracy_svm:.2f}\")\n",
    "print(f\"Precision SVM: {precision_svm:.2f}\")\n",
    "print(f\"AUC SVM: {AUC_svm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabefd6-3a14-43d6-9fe2-1b468e0fe5af",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3145e8-646d-48be-b2b4-3e51ceb98ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the best parameters\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'max_depth': randint(10,44)}\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539dc35-ebf1-48d0-b34e-60bba541aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search = RandomizedSearchCV(rf, param_distributions = param_dist, n_iter=10, cv=5, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59b87f-065a-448e-b6cd-d33bf0e3c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search.fit(X_resampled, y_resampled['Transaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4efefcf-4b13-4ed2-8f24-98e0e85f9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c16db4-099d-4bfd-899d-99ce3dd3e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 300, max_depth = 10, random_state = 42)\n",
    "y_pred_rf = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3b413-3615-40d2-b79b-8ea2d7279dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing RF\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "fpr_rf, tpr_rf, thresholds = roc_curve(y_test , y_pred_rf)\n",
    "AUC_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "print(f\"Accuracy RF: {accuracy_rf}\")\n",
    "print(f\"Precision RF: {precision_rf}\")\n",
    "print(f\"AUC RF: {AUC_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81accd31-a519-444f-bd92-095d9e44149f",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ab6e1-3610-4138-a607-59ba15f57fea",
   "metadata": {},
   "source": [
    "References: \n",
    "https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d5b46-c03f-412d-b3fb-2bf78b07bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a55a5-bca4-4212-b2a8-5ff02be043b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b4e60-13c3-403b-a13c-9ba06334a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.read_csv('/content/drive/My Drive/Thesis Backup/df_NYSE_4_short.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cec313-be44-469f-b950-02b985803968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list\n",
    "texts = []\n",
    "for i in df_model['combined']:\n",
    "    texts.append(i)\n",
    "\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da378cd-eee4-40b3-a890-8894bd194f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the model\n",
    "embeddings =[]\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    flattened_embedding = torch.mean(last_hidden_states, dim=1).flatten().detach().cpu().numpy()\n",
    "    embeddings.append(flattened_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f4f28-2c78-44f3-9b01-6a58fb685190",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = [emb.tolist() for emb in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddd451-1871-43e9-b63b-0679ec98df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the embeddings to a dataframe\n",
    "df_model['Bert']= embeddings_list\n",
    "print(df_model.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e4da7-7586-4719-a311-fd783f404fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv('/content/drive/My Drive/Thesis Backup/df_NYSE_BERT_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28934b00-e7cf-4379-841a-402da72f2733",
   "metadata": {},
   "source": [
    "Now follow the same steps as for the text-embedding-ada-002 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa6337-8a6c-4254-8551-914a10d0fb5b",
   "metadata": {},
   "source": [
    "## Adding a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb96c1-2142-4798-90f1-7448de4a61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the dataframe\n",
    "df_combined = pd.read_csv('c:/Users/xxxxx/Downloads/df_NYSE_1_short.csv')\n",
    "\n",
    "df_combined['prompt'] = \"Let's think step by step if this company will be acquired.\" + df_combined['combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6aaea-fb49-490b-a72e-970b5d4dae3a",
   "metadata": {},
   "source": [
    "Now follow the same steps as for the text-embedding-ada-002 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bdf101-c8ed-4b64-9d47-6d553bc5d6ce",
   "metadata": {},
   "source": [
    "## Histograms, ROC curves and Confusion Matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e8059-a714-4cb9-b2f1-f04b8d15ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram Targets\n",
    "plt.hist(df_acquired['Tokens'], bins=25, color = 'darkblue')\n",
    "plt.xlabel(\"Token Size\")\n",
    "plt.ylabel(\"Number of Filings\")\n",
    "plt.savefig(\"/content/drive/My Drive/Thesis Backup/HistogramA.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22a6d6-4833-4d9c-8322-5b5323e88b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr_svm, tpr_svm, color='blue', lw=2, label='SVM (area = %0.2f)' % AUC_svm)\n",
    "plt.plot(fpr_lr, tpr_lr, color='green', lw=2, label='Logistic Regression (area = %0.2f)' % AUC_lr)\n",
    "plt.plot(fpr_rf, tpr_rf, color='red', lw=2, label='Random Forest (area = %0.2f)' % AUC_rf)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"/content/drive/My Drive/Thesis Backup/ROC_curves_ada.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302614b1-f065-4d41-bd36-3e2e621b1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "sns.heatmap(cm_lr, annot = True, fmt=\"d\", cmap = \"Greens\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"/content/drive/My Drive/Thesis Backup/confusion_matrix_lr_ada.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
